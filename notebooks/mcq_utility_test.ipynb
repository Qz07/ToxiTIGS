{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bfd61e-4c47-4deb-b65b-08f5a3b797c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc62c20-962b-4c61-8020-917eb9ab7060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbde23ba0ee04d4ba5e6ba1a7797c928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488b38bce5ba44919804d74c9f4471ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458709ee4e0149eeac5f45447bdf9b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3b169cbafb4b4b98408343b54f1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03310f152e044c1f8e3ec5c2c7e62d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbaa91ed52d440f97fff3edd71216b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1d304b4a7d496db99d319d1ebc59d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c7e804-0947-4f61-ac72-ac507e68d041",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_lm(model_name_or_path: str, device: str | None = None, dtype=None):\n",
    "    \"\"\"\n",
    "    model_name_or_path can be:\n",
    "      - Hugging Face hub id: \"gpt2\", \"EleutherAI/gpt-j-6B\", etc.\n",
    "      - local folder created by model.save_pretrained(...), containing config.json + weights\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if dtype is None:\n",
    "        # good default on modern NVIDIA GPUs\n",
    "        dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    # GPT-2-like models often don't have pad_token; set to eos for batching\n",
    "    if tok.pad_token is None and tok.eos_token is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "\n",
    "    # If on CPU, explicitly move\n",
    "    if device != \"cuda\":\n",
    "        model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "# Example\n",
    "tokenizer, model = load_lm(\"gpt2\")  # or \"/path/to/your/checkpoint_dir\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e635a4-9434-4a32-93f7-e80482ef88bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '61fe6e879ff18686d7552425a36344c8',\n",
       " 'question': 'Sammy wanted to go to where the people were.  Where might he go?',\n",
       " 'question_concept': 'people',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['race track',\n",
       "   'populated areas',\n",
       "   'the desert',\n",
       "   'apartment',\n",
       "   'roadblock']},\n",
       " 'answerKey': 'B'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "953046ad-6487-4383-af7a-fe6b9fde62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ds['train'][1]['question']\n",
    "choices = \"\\n\".join(f\"{l} {t}\" for l, t in zip(ds['train'][1]['choices'][\"label\"], ds['train'][1]['choices'][\"text\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a622a8-2016-4451-9061-fb5d0e33989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57fc959b-55a6-44a1-9878-f87cdd549ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fmt = \"\"\"Answer the given question.\n",
    "{question}\n",
    "\n",
    "{choices}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "449aadcf-9810-4b4b-b52b-2ba4c8d4e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the given question.\n",
      "Sammy wanted to go to where the people were.  Where might he go?\n",
      "\n",
      "A race track\n",
      "B populated areas\n",
      "C the desert\n",
      "D apartment\n",
      "E roadblock\n",
      "F the forest\n",
      "\n",
      "G a place to run\n",
      "\n",
      "H a place to sleep\n",
      "\n",
      "I am not very good at answering all of these questions, especially when you don't know what to ask yourself. For some reason, though, my brain seems to respond to more questions than answers. \n",
      "\n",
      "When I ask my question in these situations, I immediately hear the answer.  The answer\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_fmt.format(question=question, choices=choices)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a36a613-ec5d-4cbe-aba5-7969273fd8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.44.2\n",
      "tokenizers: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import transformers, tokenizers\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"tokenizers:\", tokenizers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3308e0-80fc-43e4-96c9-e699490a66f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
